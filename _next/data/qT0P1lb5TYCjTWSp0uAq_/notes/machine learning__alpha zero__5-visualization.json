{"pageProps":{"note_id":"machine learning__alpha zero__5-visualization","noteData":{"metadata":{"title":"5. Visualization","date":"2024-01-21"},"content":"<h3>BUGS!!!</h3>\n<p>I tried visualising the neural net outputs using matplotlib and I noticed the\ntraining data was all wrong!!! There were two bugs:</p>\n<ol>\n<li>\n<p>Board class is mutable... so when we ran <code>generate_random_game</code>, the board\nstate saved in <code>history</code> would be only a pointer to the current board. So\nas the game progressed the history instead of looking like\n<code>[(P0, M0), (P1, M1), ... (Pn, Mn)]</code> would look like\n<code>[(Pn, M0), (Pn, M1), ... (Pn, Mn)]</code>.</p>\n</li>\n<li>\n<p>In <code>chess_utils.py</code>, <code>moves_to_tensor</code>, I had assumed that <code>torch.BoolTensor</code>\nwould give me a tensor with all <code>False</code> in the beginning. That was incorrect,\nit instead gives me a tensor filled with <code>True</code> and <code>False</code> randomly.\n(Probably garbage values because we didn't initialise the buffer with\nanything). So we had to use <code>torch.zeros</code> instead.</p>\n</li>\n</ol>\n<p>Bug fixes:</p>\n<pre><code class=\"hljs language-diff\">  def generate_random_game() -> List[Tuple[chess.Board, List[chess.Move]]]:\n      board = chess.Board()\n      history = []\n      while not board.is_game_over():\n          # print(board)\n          valid_moves = list(board.generate_legal_moves())\n          # print(valid_moves)\n          random_move = random.choice(valid_moves)\n<span class=\"hljs-addition\">+         history.append((chess.Board(board.fen()), valid_moves))</span>\n<span class=\"hljs-deletion\">-         history.append((board, valid_moves))</span>\n          board.push(random_move)\n      return history\n</code></pre>\n<pre><code class=\"hljs language-diff\">  def moves_to_tensor(moves: List[chess.Move]) -> torch.Tensor:\n<span class=\"hljs-addition\">+     moves_tensor = torch.BoolTensor(64*64)</span>\n<span class=\"hljs-deletion\">-     moves_tensor = torch.zeros(64*64)</span>\n      valid_actions = [move_to_action(move) for move in moves]\n      moves_tensor[valid_actions] = 1\n      return moves_tensor\n</code></pre>\n<p>After fixing these two bugs, the neural nets started training much better!</p>\n<p>Before bug fix:</p>\n<pre><code>run_num,OPTIMIZER,NUM_TRAINING_EXAMPLES,BATCH_SIZE,NUM_EPOCHS,LEARNING_RATE,final_train_loss,final_test_loss,running_time\n1,ADAM,10000,128,20,1,0.0917012087764248,0.0948999347165227,9.442247152328491\n</code></pre>\n<p>After bug fix:</p>\n<pre><code>run_num,OPTIMIZER,NUM_TRAINING_EXAMPLES,BATCH_SIZE,NUM_EPOCHS,LEARNING_RATE,final_train_loss,final_test_loss,running_time\n1,ADAM,10000,128,20,1,0.010415336841510402,0.012209153734147549,9.533048629760742\n</code></pre>\n<p><img src=\"/notes/loss_after_data_fix.png\" alt=\"loss plot after bug fix\"></p>\n<h3>Visualizations</h3>\n<p>Ok, after training the network again let's now look at the visualizations again.\nThis is the code I used to visualise the networks:</p>\n<pre><code class=\"hljs language-py\"><span class=\"hljs-comment\"># To plot the board tensor</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">plot_pos_tensor</span>(<span class=\"hljs-params\">tensor: torch.Tensor</span>):\n    fig, axs = plt.subplots(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">7</span>, figsize=(<span class=\"hljs-number\">15</span>, <span class=\"hljs-number\">5</span>))\n    channel_names = [<span class=\"hljs-string\">'TURN'</span>, <span class=\"hljs-string\">'PAWN'</span>, <span class=\"hljs-string\">'KNIGHT'</span>, <span class=\"hljs-string\">'BISHOP'</span>, <span class=\"hljs-string\">'ROOK'</span>, <span class=\"hljs-string\">'QUEEN'</span>, <span class=\"hljs-string\">'KING'</span>]\n\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">7</span>):\n        axs[i].imshow(tensor[i, :, :], cmap=<span class=\"hljs-string\">'gray'</span>, vmin=-<span class=\"hljs-number\">1</span>, vmax=<span class=\"hljs-number\">1</span>, origin=<span class=\"hljs-string\">\"lower\"</span>)\n        axs[i].set_title(channel_names[i])\n\n    plt.show()\n\n<span class=\"hljs-comment\"># To plot the moves tensor</span>\n<span class=\"hljs-comment\"># This will plot the moves tensor of length 4096 as a 64x64 image.</span>\n<span class=\"hljs-comment\"># The image can be thought of as a big chessboard with small chessboards on</span>\n<span class=\"hljs-comment\"># each square. The bigger square represents the start square and the smaller</span>\n<span class=\"hljs-comment\"># square represents the end square for each move.</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">plot_move_set</span>(<span class=\"hljs-params\">moves: torch.Tensor</span>):\n    moves_view = torch.zeros(<span class=\"hljs-number\">64</span>, <span class=\"hljs-number\">64</span>)\n    <span class=\"hljs-keyword\">for</span> start_row <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">8</span>):\n        <span class=\"hljs-keyword\">for</span> start_col <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">8</span>):\n            <span class=\"hljs-keyword\">for</span> end_row <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">8</span>):\n                <span class=\"hljs-keyword\">for</span> end_col <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">8</span>):\n                    action_num = (start_row+start_col*<span class=\"hljs-number\">8</span>)*<span class=\"hljs-number\">64</span>+(end_row+end_col*<span class=\"hljs-number\">8</span>)\n                    moves_view[start_col*<span class=\"hljs-number\">8</span>+end_col][start_row*<span class=\"hljs-number\">8</span>+end_row] = moves[action_num]\n    plt.imshow(moves_view, origin=<span class=\"hljs-string\">\"lower\"</span>, vmin=<span class=\"hljs-number\">0</span>, vmax=<span class=\"hljs-number\">1</span>)\n    plt.show()\n\n<span class=\"hljs-comment\"># To plot the top k moves as arrows on an svg chess board</span>\n<span class=\"hljs-comment\"># (use in jupyter notebooks)</span>\n<span class=\"hljs-comment\"># The opacity of the arrow is proportional to the move probability.</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">plot_board_moves</span>(<span class=\"hljs-params\">board: chess.Board, probs: torch.Tensor, k=<span class=\"hljs-number\">20</span></span>):\n    arrows = []\n    probs, actions = torch.topk(probs, k)\n    <span class=\"hljs-keyword\">for</span> prob, action <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">zip</span>(probs, actions):\n        arrow_color = <span class=\"hljs-string\">f\"#009900<span class=\"hljs-subst\">{<span class=\"hljs-built_in\">int</span>(prob*<span class=\"hljs-number\">128</span>):02x}</span>\"</span>\n        move = action_to_move(action.item(), board)\n        <span class=\"hljs-built_in\">print</span>(move, prob)\n        arrows.append(chess.svg.Arrow(move.from_square, move.to_square, color=arrow_color))\n    <span class=\"hljs-keyword\">return</span> chess.svg.board(board, arrows=arrows)\n</code></pre>\n<h4>Start position</h4>\n<p>Here's what the start position looks like:</p>\n<p><img src=\"/notes/start-position.png\" alt=\"start position tensor with 7 planes\"></p>\n<p>And here is a representation of the valid moves of the start position:</p>\n<p><img src=\"/notes/start-moves.png\" alt=\"valid moves image for start position\"></p>\n<p>Note that this is a 64x64 image and it can be thought of as a big chessboard\nwith small chessboards on each square. The bigger square represents the start\nsquare and the smaller square represents the end square for each move.</p>\n<p>We can see the pawns having 2 moves each and the knights also having 2 possible\nmoves each in the starting position. All other pieces are blocked.</p>\n<h4>Position 1</h4>\n<p>Let's look at a certain random position. It's Black's turn to play.</p>\n<p><img src=\"/notes/board-position-141.svg\" alt=\"board at position 141\"></p>\n<p><img src=\"/notes/random-position-141.png\" alt=\"board tensor for position 141\"></p>\n<p>This should be the correct answer for the possible moves:</p>\n<p><img src=\"/notes/moves-position-141.png\" alt=\"valid moves position 141\"></p>\n<p>And if we plot what the neural network outputs, this is what we get (the\npredicted probabilities for the possible moves):</p>\n<p><img src=\"/notes/moves-predicted-141.png\" alt=\"predicted moves position 141\"></p>\n<p>Here's how it looks like represented on a chessboard with arrows. (The opacity\nof the arrow is proportional to move probability.)</p>\n<p><img src=\"/notes/board-moves-predicted-141.svg\" alt=\"predicted moves with arrows position 141\"></p>\n<p>We can see its mostly correctly predicting the moves for the queen and bishop,\nalthough it's still predicting a bit of probability for the black bishop going past\nthe white bishop. It's also struggling a bit to predict the rook moves which are totally\nvalid (not sure why) but there is still some probability predicted for those.\nAlso the black king shouldn't be able to move anywhere (check), and the net\ndefinitely is showing low probabilities for those moves.</p>\n<p>So the net definitely understands some sort of concept that the king can't move\ninto check.</p>\n<h4>Position 2</h4>\n<p>Here's another slightly trickier position. This time it's tricky to predict\nthe right moves because the king is in check and there is really only one\nmove that is valid in this position (bishop to d2 to block the check).</p>\n<p><img src=\"/notes/board-position-142.svg\" alt=\"board at position 142\"></p>\n<p>Board tensor:</p>\n<p><img src=\"/notes/random-position-142.png\" alt=\"board tensor for position 142\"></p>\n<p>Valid moves (only one pixel is ON):</p>\n<p><img src=\"/notes/moves-position-142.png\" alt=\"valid moves position 142\"></p>\n<p>This is what the neural net predicts:</p>\n<p><img src=\"/notes/moves-predicted-142.png\" alt=\"predicted moves position 142\"></p>\n<p><img src=\"/notes/board-moves-predicted-142.svg\" alt=\"predicted moves with arrows position 142\"></p>\n<p>Here are the raw probabilities for each move:</p>\n<pre><code>c3c4 tensor(0.6879)\ne3d2 tensor(0.6831)\ne5f6 tensor(0.6757)\nd4d3 tensor(0.6654)\nh5h6 tensor(0.6511)\nh1h3 tensor(0.6164)\nh1h2 tensor(0.5898)\nh1h4 tensor(0.4512)\ne3f4 tensor(0.3785)\ne3f2 tensor(0.3347)\n</code></pre>\n<p>The correct move is e3d2, it has the second highest probability. However the\nnet is still struggling to understand this position (as I would have expected).\nIt is only a single layer net so far, probably more layers would be required\nto understand how to detect (and block) checks. Intuitively, it takes one mental\nstep to generate valid moves, it takes one more step to detect checks, and yet\none more step to realise that we are in check and we need to block it.</p>\n<p>Anyway, I've committed the code so far at <a href=\"https://github.com/krkartikay/chess-sl/tree/a7f6106b7257a0ccb28a48624b76e0413b9b0b24\">commit <code>a7f6106</code></a>. Make sure to check out\nthe visualisations in the <a href=\"https://github.com/krkartikay/chess-sl/blob/a7f6106b7257a0ccb28a48624b76e0413b9b0b24/visualizations.ipynb\">jupyter notebook!</a>.</p>\n<p>Okay so what next? Next I'll try to develop an evaluation framework to\n(a) test the network by 'live' play and measure number of moves played,\n(b) measure metrics like precision/recall\n(c) Try more neural net architectures to drive those metrics up!</p>"},"allNotesIndex":{"base_name":"notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes","notes_data":[{"id":"quotes","metadata":{"title":"Quotes","date":"2023-03-21"}}],"directories":[{"base_name":"book notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes","notes_data":[{"id":"dokkodo","metadata":{"title":"Dokkodo","date":"2023-03-23"}},{"id":"if","metadata":{"title":"\"If\"","date":"2023-03-27"}},{"id":"worry","metadata":{"title":"Worry","date":"2023-03-29"}}],"directories":[{"base_name":"the bhagvad gita","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes/the bhagvad gita","notes_data":[{"id":"chapter_1","metadata":{"title":"Chapter 1","date":"2023-04-06"}},{"id":"chapter_2","metadata":{"title":"Chapter 2","date":"2023-04-08"}},{"id":"index","metadata":{"title":"The Bhagvad Gita","date":"2023-04-06"}}],"directories":[]}]},{"base_name":"economics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics","notes_data":[],"directories":[{"base_name":"microeconomics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics/microeconomics","notes_data":[{"id":"index","metadata":{"title":"Principles of Microeconomics","date":"2023-03-23"}},{"id":"lecture_1","metadata":{"title":"Lecture 1","date":"2023-03-23"}},{"id":"lecture_2","metadata":{"title":"Lecture 2","date":"2023-03-23"}},{"id":"lecture_3","metadata":{"title":"Lecture 3","date":"2023-03-23"}},{"id":"lecture_4","metadata":{"title":"Lecture 4","date":"2023-04-01"}},{"id":"lecture_5","metadata":{"title":"Lecture 5","date":"2023-04-01"}},{"id":"lecture_6","metadata":{"title":"Lecture 6","date":"2023-04-01"}}],"directories":[]}]},{"base_name":"machine learning","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/machine learning","notes_data":[{"id":"cpp-grad","metadata":{"title":"cpp-grad","date":"2023-04-17"}},{"id":"karpathy-lectures","metadata":{"title":"Andrey Karpathy's Lectures","date":"2023-05-14"}},{"id":"study-plan","metadata":{"title":"ML Study plan","date":"2024-01-14"}}],"directories":[{"base_name":"alpha zero","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/machine learning/alpha zero","notes_data":[{"id":"1-the-start","metadata":{"title":"1. The Start","date":"2024-01-14"}},{"id":"2-initial-plan","metadata":{"title":"2. Initial plan","date":"2024-01-14"}},{"id":"3-supervised-learning","metadata":{"title":"3. Supervised Learning","date":"2024-01-15"}},{"id":"4-experimentation","metadata":{"title":"4. Experimentation Framework","date":"2024-01-20"}},{"id":"5-visualization","metadata":{"title":"5. Visualization","date":"2024-01-21"}},{"id":"_notes","metadata":{"title":"_notes"}}],"directories":[]}]}]}},"__N_SSG":true}