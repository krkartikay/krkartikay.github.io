<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>4. Experimentation Framework</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/584705cbbef33b49.css" as="style"/><link rel="stylesheet" href="/_next/static/css/584705cbbef33b49.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-83cebdb887f48834.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb98586f475796ed.js" defer=""></script><script src="/_next/static/chunks/247-fb3f3c148c859656.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bnote_id%5D-774c61eb2a4693cc.js" defer=""></script><script src="/_next/static/0cQi27foYwZlvrxIm4tXk/_buildManifest.js" defer=""></script><script src="/_next/static/0cQi27foYwZlvrxIm4tXk/_ssgManifest.js" defer=""></script></head><body class="dark:bg-black dark:bg-opacity-95"><div id="__next"><div><nav class="md:sticky md:top-0 md:z-40 shadow-sm p-2 bg-white dark:bg-black dark:shadow-lg dark:shadow-stone-800"><div class="flex items-center"><a class="flex" href="/"><img alt="Profile pic" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="rounded-full h-20 w-20 mx-2" style="color:transparent" src="/pfp_blank.png"/><h1 class="self-center text-xl text-stone-400 font-mono dark:text-stone-50 dark:drop-shadow-2xl dark:shadow-white dark:hover:text-stone-200">krkartikay&#x27;s notes</h1></a></div></nav><div class="flex flex-row flex-wrap"><aside class="md:w-1/4 text-stone-500 dark:text-stone-200"><div class="max-w-md mx-auto py-8"><a href="/"><h2 class="p-8 text-lg text-stone-600 hover:bg-stone-100 dark:text-stone-200 dark:hover:bg-stone-800 dark:hover:text-stone-200 ">Notes</h2></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Book Notes</li></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__if"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">&quot;If&quot;</li></a><a href="/notes/book%20notes__dokkodo"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Dokkodo</li></a><a href="/notes/book%20notes__the%20bhagvad%20gita__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">The Bhagvad Gita</li></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__the%20bhagvad%20gita__chapter_1"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Chapter 1</li></a><a href="/notes/book%20notes__the%20bhagvad%20gita__chapter_2"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Chapter 2</li></a></ul><a href="/notes/book%20notes__worry"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Worry</li></a></ul><a href="/notes/economics__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Economics</li></a><ul class="list-disc pl-8"><a href="/notes/economics__microeconomics__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Microeconomics</li></a><ul class="list-disc pl-8"><a href="/notes/economics__microeconomics__lecture_1"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 1</li></a><a href="/notes/economics__microeconomics__lecture_2"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 2</li></a><a href="/notes/economics__microeconomics__lecture_3"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 3</li></a><a href="/notes/economics__microeconomics__lecture_4"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 4</li></a><a href="/notes/economics__microeconomics__lecture_5"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 5</li></a><a href="/notes/economics__microeconomics__lecture_6"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 6</li></a></ul></ul><a href="/notes/machine%20learning__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Machine Learning</li></a><ul class="list-disc pl-8"><a href="/notes/machine%20learning__alpha%20zero__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Alpha Zero</li></a><ul class="list-disc pl-8"><a href="/notes/machine%20learning__alpha%20zero__1-the-start"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">1. The Start</li></a><a href="/notes/machine%20learning__alpha%20zero__2-initial-plan"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">2. Initial plan</li></a><a href="/notes/machine%20learning__alpha%20zero__3-supervised-learning"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">3. Supervised Learning</li></a><a href="/notes/machine%20learning__alpha%20zero__4-experimentation"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside font-bold bg-stone-50 dark:bg-stone-800">4. Experimentation Framework</li></a><a href="/notes/machine%20learning__alpha%20zero__5-visualization"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">5. Visualization</li></a><a href="/notes/machine%20learning__alpha%20zero___notes"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">_notes</li></a></ul><a href="/notes/machine%20learning__karpathy-lectures"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Andrey Karpathy&#x27;s Lectures</li></a><a href="/notes/machine%20learning__study-plan"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">ML Study plan</li></a><a href="/notes/machine%20learning__cpp-grad"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">cpp-grad</li></a></ul><a href="/notes/quotes"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Quotes</li></a></ul></div></aside><main class="md:w-3/4 border-l-2 dark:border-l-stone-900 border-l-stone-50"><div class="p-8 md:p-16"><br/><h1 class="text-4xl dark:text-white">4. Experimentation Framework</h1><p class="text-stone-400 dark:text-stone-300">2024-01-20</p><div class="prose prose-stone dark:prose-invert"><div><h3>Running the model on GPU</h3>
<p>Ok, we had left off with training the model on our small dataset of 100 games
for 10 epochs. It was taking ~1 min on my machine to run it. However I noticed
later that it was so slow because it was training on CPU!</p>
<p>To train it on the GPU, we only need a small change.</p>
<pre><code class="hljs language-diff"><span class="hljs-deletion">- chess_model = ChessModel()</span>
<span class="hljs-addition">+ # Transfer data to GPU if available</span>
<span class="hljs-addition">+ device = 'cuda' if torch.cuda.is_available() else 'cpu'</span>
<span class="hljs-addition">+ </span>
<span class="hljs-addition">+ positions = positions.to(device)</span>
<span class="hljs-addition">+ valid_moves = valid_moves.to(device)</span>
<span class="hljs-addition">+ </span>
<span class="hljs-addition">+ # Create the neural net</span>
<span class="hljs-addition">+ chess_model = ChessModel().to(device)</span>
</code></pre>
<p>The <code>.to(device)</code> function transfers data to the GPU in pytorch and the whole
computation works on the GPU after that.</p>
<pre><code>(torch) krkartikay@KartikayLegion:~/code/chess-sl$ time python train.py 
...
Epoch 10, Average Test Loss: 0.1468

real    0m6.975s
user    0m6.034s
sys     0m1.003s
</code></pre>
<p>Now it's only taking ~7sec to train the model. That's almost a 10x improvement!</p>
<p>However, an important point to be noted here is that this is working only
because our dataset is small enough to fit in the VRAM of the GPU in one go.
If the dataset did not fit in the VRAM either of two things would happen:</p>
<ol>
<li>Either the CUDA drivers fail with an out of memory error! (best-case scenario), or,</li>
<li>The training keeps running using our RAM ('shared memory') instead, but runs much more
slowly and without any warnings.</li>
</ol>
<p>In the second case we might think everything is running fine but actually there
would be an issue, and everything might be <em>much</em> slower than it can be.
Unfortunately (2) is the default behaviour of the CUDA drivers these days, but
that can be changed in the settings (later...).</p>
<p>The proper solution would really be to stream the data to the GPU per-batch, or
possibly even stream the data from disk to RAM (and from RAM to GPU) in a
multithreaded dataloader, in case the dataset is big enough to not fit in RAM.</p>
<p>However let's implement that later and focus on experimenting with the network
for now.</p>
<h3>Optimising the network</h3>
<p>We could now try out various things to optimise the network but keeping track of
how various hyperparams are affecting the quality of the network is hard if we
do it manually.</p>
<p>So we'll now develop a framework to keep track of the configs and run
experiments by changing them systematically and noting the results.</p>
<h3>Observations</h3>
<p>The most important thing while doing experiments is to keep track of
observations!</p>
<p>There's a risk of overengineering this part... For instance there are entire
teams at google which handle metric collection and monitoring. But I'll try to
keep things simple here.</p>
<p>Let's create an observer class which we can call <code>observer.record(variable)</code> on.
It will help us record and store the data.</p>
<p><a href="https://github.com/krkartikay/chess-sl/blob/main/observer.py">observer.py</a>:</p>
<pre><code class="hljs language-py"><span class="hljs-comment"># Observation framework</span>

<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Observer</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name, path=<span class="hljs-string">""</span>, suffix=<span class="hljs-string">""</span></span>):
        self.name = name
        self.suffix = suffix
        self.fullname = os.path.join(
            path, name + (<span class="hljs-string">f"_<span class="hljs-subst">{suffix}</span>"</span> <span class="hljs-keyword">if</span> suffix <span class="hljs-keyword">else</span> <span class="hljs-string">""</span>))
        self.observations = []

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">record</span>(<span class="hljs-params">self, variable</span>):
        self.observations.append(variable)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">write_csv</span>(<span class="hljs-params">self</span>):
        filename = (<span class="hljs-string">f"<span class="hljs-subst">{self.fullname}</span>.csv"</span>)
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.observations) == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">return</span>
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">"w"</span>) <span class="hljs-keyword">as</span> file_handle:
            csv_writer = csv.writer(file_handle)
            csv_writer.writerows([x] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> self.observations)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_pkl</span>(<span class="hljs-params">self</span>):
        filename = (<span class="hljs-string">f"<span class="hljs-subst">{self.fullname}</span>.pkl"</span>)
        pickle.dump(self.observations, <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">"wb"</span>))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">self</span>):
        filename = (<span class="hljs-string">f"<span class="hljs-subst">{self.fullname}</span>.png"</span>)
        plt.figure()
        plt.plot(self.observations)
        plt.savefig(filename)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">avg</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(self.observations) / <span class="hljs-built_in">len</span>(self.observations)

</code></pre>
<p>To be honest it's nothing more than an array with some functions to save it,
plot it, etc.</p>
<p>Let's see if we can plot the training and test loss with it now. (I've modified
the code a little bit to handle calling <code>record</code> on tuples/lists, and also
increased the epochs to 50.)</p>
<p>Relevant part of <code>train.py</code> now:</p>
<pre><code class="hljs language-py">sgd_optimizer = SGD(chess_model.parameters(), lr=LEARNING_RATE)

loss_observer = Observer(<span class="hljs-string">'loss'</span>, path=<span class="hljs-string">"results/"</span>)

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_EPOCHS):
    <span class="hljs-keyword">for</span> batch_num, (train_positions, train_valid_moves) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):
        sgd_optimizer.zero_grad()

        move_probs = chess_model(train_positions)
        loss = F.binary_cross_entropy(move_probs, train_valid_moves)

        loss.backward()
        sgd_optimizer.step()

        <span class="hljs-keyword">if</span> batch_num % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{batch_num+<span class="hljs-number">1</span>:3d}</span>, Loss: <span class="hljs-subst">{loss.item():<span class="hljs-number">.4</span>f}</span>"</span>)

    <span class="hljs-comment"># Test Evaluation</span>
    chess_model.<span class="hljs-built_in">eval</span>()

    total_test_loss = <span class="hljs-number">0</span>
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> test_positions, test_valid_moves <span class="hljs-keyword">in</span> test_dataloader:
            test_move_probs = chess_model(test_positions)
            test_loss = F.binary_cross_entropy(
                test_move_probs, test_valid_moves)
            total_test_loss += test_loss.item()

    last_training_loss = loss.item()
    average_test_loss = total_test_loss / <span class="hljs-built_in">len</span>(test_dataloader)
    loss_observer.record((last_training_loss, average_test_loss))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Epoch <span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>, Average Test Loss: <span class="hljs-subst">{average_test_loss:<span class="hljs-number">.4</span>f}</span>"</span>)

loss_observer.plot([<span class="hljs-string">'train_loss'</span>, <span class="hljs-string">'test_loss'</span>])
loss_observer.write_csv()
</code></pre>
<p>(Link to code so far at <a href="https://github.com/krkartikay/chess-sl/tree/09820da5afdac68cbdd3cbefe0a449cf81dccce3">commit <code>09820da</code></a>.)</p>
<p>Training Output:</p>
<pre><code>(torch) krkartikay@KartikayLegion:~/code/chess-sl$ time python train.py 
Loading data...
Loaded data. Shape: 
positions : torch.Size([34879, 7, 8, 8])
moves     : torch.Size([34879, 4096])

1/  1, Loss: 0.6941
1/ 11, Loss: 0.6910
1/ 21, Loss: 0.6880
...
50/ 91, Loss: 0.1098
50/101, Loss: 0.1038
Epoch 50, Average Test Loss: 0.1048

real    0m23.212s
user    0m21.790s
sys     0m1.442
</code></pre>
<p><code>results/loss.png</code> and <code>results/loss.csv</code> were created successfully.</p>
<p><code>loss.csv</code>:</p>
<pre><code>train_loss,test_loss
0.6621306538581848,0.6606337598391941
0.6188064217567444,0.6185613402298519
0.557918906211853,0.5588848718575069
...
</code></pre>
<p><code>loss.png</code>: <img src="/notes/loss-plot-1.png" alt="loss plot"></p>
<h3>Experimentation framework</h3>
<p>We need to optimise the neural net further! I think the loss of ~0.1 we're
getting now is still too high. We could bring it down further by tweaking
hyperparams and perhaps model architecture but let's develop a proper
framework first to run experiments.</p>
<p>I want to be able to do something like this:</p>
<pre><code class="hljs language-py">----------------------------------------------------------------------
config.py:

BATCH_SIZE = Config(default=<span class="hljs-number">128</span>, values=[<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>])
NUM_EPOCHS = Config(default=<span class="hljs-number">100</span>, dev=<span class="hljs-number">10</span>)
LEARNING_RATE = Config(default=<span class="hljs-number">0.1</span>, values=[<span class="hljs-number">0.01</span>, <span class="hljs-number">0.03</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>])
----------------------------------------------------------------------
main.py:

<span class="hljs-keyword">def</span> <span class="hljs-title function_">run_training</span>():
    train_model()

run_experiment(variables=[BATCH_SIZE, LEARNING_RATE],
               function=run_training,
               time_limit=<span class="hljs-string">"5 mins"</span>)

----------------------------------------------------------------------
train.py:

<span class="hljs-keyword">from</span> config <span class="hljs-keyword">import</span> NUM_EPOCHS, LEARNING_RATE, BATCH_SIZE

<span class="hljs-keyword">def</span> <span class="hljs-title function_">train_model</span>():
    ...
    optim = SGD(model.parameters(), learning_rate=LEARNING_RATE.get())
    
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(NUM_EPOCHS.get()):
        ...

----------------------------------------------------------------------
</code></pre>
<p>That is, I specify all possible values of my hyperparameters and
<code>run_experiment</code> basically tries out all the possible combinations of the
variables I specify and stores all the results. So that I could see what the
effect of each variable is on the training, and also find out the optimal values
of hyperparameters which are giving me the best loss. I also want it to have a
<code>dev</code> mode where it runs experiments on a smaller scale which I can use to test
my code before doing a full training run.</p>
<p>Okay... implemented this... here's some of the relevant parts of the code:</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Experiment</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, variables=[], dev_mode=<span class="hljs-literal">False</span></span>):
        self.variables: <span class="hljs-type">List</span>[Config] = variables
        self.dev_mode: <span class="hljs-built_in">bool</span> = dev_mode
        ...
        self.run_number: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>
        self.selected_values: <span class="hljs-type">Dict</span>[Config, <span class="hljs-type">Any</span>] = {}
        self.all_configs_results: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>] = {}
        <span class="hljs-keyword">for</span> config <span class="hljs-keyword">in</span> ALL_CONFIGS:
            config.expt = self
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_config</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-keyword">if</span> config <span class="hljs-keyword">in</span> self.selected_values:
            <span class="hljs-keyword">return</span> self.selected_values[config]
        <span class="hljs-keyword">elif</span> self.dev_mode:
            <span class="hljs-keyword">return</span> config.dev
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> config.default

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_experiment</span>(<span class="hljs-params">self,
                       function=<span class="hljs-literal">None</span>,
                       time_limit: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span></span>):
        value_combinations = itertools.product(
            *[v.values <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> self.variables])
        <span class="hljs-keyword">for</span> values <span class="hljs-keyword">in</span> value_combinations:
            self.run_experiment_with_values(function, values)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_experiment_with_values</span>(<span class="hljs-params">self, function, values</span>):
        self.selected_values = {
            self.variables[i].name: values[i]
            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.variables))}
        <span class="hljs-comment"># run_experiment will run this function with selected values of</span>
        <span class="hljs-comment"># variables</span>
        self.run_number += <span class="hljs-number">1</span>
        <span class="hljs-built_in">print</span>()
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"==================================================="</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Experiment run <span class="hljs-subst">{self.run_number}</span>"</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"---------------------------------------------------"</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Config for this run:\n<span class="hljs-subst">{self.selected_values}</span>"</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"==================================================="</span>)
        start_time = time.time()
        results = function()
        end_time = time.time()
        time_taken = end_time - start_time
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"==================================================="</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Run finished in <span class="hljs-subst">{time_taken:<span class="hljs-number">3.2</span>f}</span>sec."</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"==================================================="</span>)
        <span class="hljs-built_in">print</span>()
        self.all_configs_results.append({<span class="hljs-string">'run_num'</span>: self.run_number})
        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> self.selected_values.items():
            self.all_configs_results[-<span class="hljs-number">1</span>][key] = value
        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> results.items():
            self.all_configs_results[-<span class="hljs-number">1</span>][key] = value
        self.all_configs_results[-<span class="hljs-number">1</span>][<span class="hljs-string">'running_time'</span>] = time_taken
        <span class="hljs-keyword">for</span> obs <span class="hljs-keyword">in</span> ALL_OBSERVERS:
            obs.set_path(path=self.results_path,
                         suffix=<span class="hljs-string">f"<span class="hljs-subst">{self.run_number:03d}</span>"</span>)
            obs.plot()
            obs.write_csv()
        self.save_results()
</code></pre>
<p>Link to the full code at <a href="https://github.com/krkartikay/chess-sl/tree/1547c2a2d19a4b557eb044688467daca2c57c12b">commit <code>1547c2a</code></a>.</p>
<p>It's not really the best code I've written... I'm sure the design isn't as good
as it could have been... but at least it works.</p>
<p>Here's what the results look like:</p>
<p><img src="/notes/experiment-results-1.png" alt="experiment results csv and loss graph"></p>
<h3>Analysing the results</h3>
<p>Loading up the <code>results.csv</code> in excel and applying conditional formatting for
easier visualisation, here's what I get:</p>
<p><img src="/notes/experiment-results-2.png" alt="experiment results spreadsheet"></p>
<p>What're the main conclusions we can draw from this data? The main things I can
notice are:</p>
<ul>
<li>Lower batch sizes take longer to compute (obvious)</li>
<li>Smaller batch sizes train faster (lower loss even with low LR)</li>
<li>Larger batch sizes requiring higher LR to converge (gradient competition?)</li>
<li>Theoretically I would've guessed lower batch sizes to overfit more easily,
and it kinda looks like it's going in that direction in run #6 &#x26; #7, train loss
becoming slightly lower than test loss... (although I need to run it for
more epochs to confirm this...)</li>
<li>Conversely larger batch sizes should've generalised a bit more (run #20?)</li>
<li>Final loss is around ~0.1-0.09 in all cases.</li>
</ul>
<p>That's it for now. Next time we can try:</p>
<ul>
<li>Improve neural net architecture: adding conv layers etc.</li>
<li>Evaluate the model by making it play games?</li>
<li>More training data</li>
<li>Visualisations...</li>
</ul></div></div></div></main></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note_id":"machine learning__alpha zero__4-experimentation","noteData":{"metadata":{"title":"4. Experimentation Framework","date":"2024-01-20"},"content":"\u003ch3\u003eRunning the model on GPU\u003c/h3\u003e\n\u003cp\u003eOk, we had left off with training the model on our small dataset of 100 games\nfor 10 epochs. It was taking ~1 min on my machine to run it. However I noticed\nlater that it was so slow because it was training on CPU!\u003c/p\u003e\n\u003cp\u003eTo train it on the GPU, we only need a small change.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-diff\"\u003e\u003cspan class=\"hljs-deletion\"\u003e- chess_model = ChessModel()\u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ # Transfer data to GPU if available\u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ device = 'cuda' if torch.cuda.is_available() else 'cpu'\u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ \u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ positions = positions.to(device)\u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ valid_moves = valid_moves.to(device)\u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ \u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ # Create the neural net\u003c/span\u003e\n\u003cspan class=\"hljs-addition\"\u003e+ chess_model = ChessModel().to(device)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003e.to(device)\u003c/code\u003e function transfers data to the GPU in pytorch and the whole\ncomputation works on the GPU after that.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e(torch) krkartikay@KartikayLegion:~/code/chess-sl$ time python train.py \n...\nEpoch 10, Average Test Loss: 0.1468\n\nreal    0m6.975s\nuser    0m6.034s\nsys     0m1.003s\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow it's only taking ~7sec to train the model. That's almost a 10x improvement!\u003c/p\u003e\n\u003cp\u003eHowever, an important point to be noted here is that this is working only\nbecause our dataset is small enough to fit in the VRAM of the GPU in one go.\nIf the dataset did not fit in the VRAM either of two things would happen:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEither the CUDA drivers fail with an out of memory error! (best-case scenario), or,\u003c/li\u003e\n\u003cli\u003eThe training keeps running using our RAM ('shared memory') instead, but runs much more\nslowly and without any warnings.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn the second case we might think everything is running fine but actually there\nwould be an issue, and everything might be \u003cem\u003emuch\u003c/em\u003e slower than it can be.\nUnfortunately (2) is the default behaviour of the CUDA drivers these days, but\nthat can be changed in the settings (later...).\u003c/p\u003e\n\u003cp\u003eThe proper solution would really be to stream the data to the GPU per-batch, or\npossibly even stream the data from disk to RAM (and from RAM to GPU) in a\nmultithreaded dataloader, in case the dataset is big enough to not fit in RAM.\u003c/p\u003e\n\u003cp\u003eHowever let's implement that later and focus on experimenting with the network\nfor now.\u003c/p\u003e\n\u003ch3\u003eOptimising the network\u003c/h3\u003e\n\u003cp\u003eWe could now try out various things to optimise the network but keeping track of\nhow various hyperparams are affecting the quality of the network is hard if we\ndo it manually.\u003c/p\u003e\n\u003cp\u003eSo we'll now develop a framework to keep track of the configs and run\nexperiments by changing them systematically and noting the results.\u003c/p\u003e\n\u003ch3\u003eObservations\u003c/h3\u003e\n\u003cp\u003eThe most important thing while doing experiments is to keep track of\nobservations!\u003c/p\u003e\n\u003cp\u003eThere's a risk of overengineering this part... For instance there are entire\nteams at google which handle metric collection and monitoring. But I'll try to\nkeep things simple here.\u003c/p\u003e\n\u003cp\u003eLet's create an observer class which we can call \u003ccode\u003eobserver.record(variable)\u003c/code\u003e on.\nIt will help us record and store the data.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/krkartikay/chess-sl/blob/main/observer.py\"\u003eobserver.py\u003c/a\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-py\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Observation framework\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e csv\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pickle\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e matplotlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eObserver\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, name, path=\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e, suffix=\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003c/span\u003e):\n        self.name = name\n        self.suffix = suffix\n        self.fullname = os.path.join(\n            path, name + (\u003cspan class=\"hljs-string\"\u003ef\"_\u003cspan class=\"hljs-subst\"\u003e{suffix}\u003c/span\u003e\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e suffix \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e))\n        self.observations = []\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erecord\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, variable\u003c/span\u003e):\n        self.observations.append(variable)\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ewrite_csv\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        filename = (\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{self.fullname}\u003c/span\u003e.csv\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(self.observations) == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(filename, \u003cspan class=\"hljs-string\"\u003e\"w\"\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e file_handle:\n            csv_writer = csv.writer(file_handle)\n            csv_writer.writerows([x] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.observations)\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esave_pkl\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        filename = (\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{self.fullname}\u003c/span\u003e.pkl\"\u003c/span\u003e)\n        pickle.dump(self.observations, \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(filename, \u003cspan class=\"hljs-string\"\u003e\"wb\"\u003c/span\u003e))\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        filename = (\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{self.fullname}\u003c/span\u003e.png\"\u003c/span\u003e)\n        plt.figure()\n        plt.plot(self.observations)\n        plt.savefig(filename)\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eavg\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(self.observations) / \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(self.observations)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo be honest it's nothing more than an array with some functions to save it,\nplot it, etc.\u003c/p\u003e\n\u003cp\u003eLet's see if we can plot the training and test loss with it now. (I've modified\nthe code a little bit to handle calling \u003ccode\u003erecord\u003c/code\u003e on tuples/lists, and also\nincreased the epochs to 50.)\u003c/p\u003e\n\u003cp\u003eRelevant part of \u003ccode\u003etrain.py\u003c/code\u003e now:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-py\"\u003esgd_optimizer = SGD(chess_model.parameters(), lr=LEARNING_RATE)\n\nloss_observer = Observer(\u003cspan class=\"hljs-string\"\u003e'loss'\u003c/span\u003e, path=\u003cspan class=\"hljs-string\"\u003e\"results/\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(NUM_EPOCHS):\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e batch_num, (train_positions, train_valid_moves) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(train_dataloader):\n        sgd_optimizer.zero_grad()\n\n        move_probs = chess_model(train_positions)\n        loss = F.binary_cross_entropy(move_probs, train_valid_moves)\n\n        loss.backward()\n        sgd_optimizer.step()\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e batch_num % \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n            \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{epoch+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e}\u003c/span\u003e/\u003cspan class=\"hljs-subst\"\u003e{batch_num+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:3d}\u003c/span\u003e, Loss: \u003cspan class=\"hljs-subst\"\u003e{loss.item():\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-comment\"\u003e# Test Evaluation\u003c/span\u003e\n    chess_model.\u003cspan class=\"hljs-built_in\"\u003eeval\u003c/span\u003e()\n\n    total_test_loss = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e torch.no_grad():\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e test_positions, test_valid_moves \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e test_dataloader:\n            test_move_probs = chess_model(test_positions)\n            test_loss = F.binary_cross_entropy(\n                test_move_probs, test_valid_moves)\n            total_test_loss += test_loss.item()\n\n    last_training_loss = loss.item()\n    average_test_loss = total_test_loss / \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(test_dataloader)\n    loss_observer.record((last_training_loss, average_test_loss))\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Epoch \u003cspan class=\"hljs-subst\"\u003e{epoch+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e}\u003c/span\u003e, Average Test Loss: \u003cspan class=\"hljs-subst\"\u003e{average_test_loss:\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e)\n\nloss_observer.plot([\u003cspan class=\"hljs-string\"\u003e'train_loss'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'test_loss'\u003c/span\u003e])\nloss_observer.write_csv()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(Link to code so far at \u003ca href=\"https://github.com/krkartikay/chess-sl/tree/09820da5afdac68cbdd3cbefe0a449cf81dccce3\"\u003ecommit \u003ccode\u003e09820da\u003c/code\u003e\u003c/a\u003e.)\u003c/p\u003e\n\u003cp\u003eTraining Output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e(torch) krkartikay@KartikayLegion:~/code/chess-sl$ time python train.py \nLoading data...\nLoaded data. Shape: \npositions : torch.Size([34879, 7, 8, 8])\nmoves     : torch.Size([34879, 4096])\n\n1/  1, Loss: 0.6941\n1/ 11, Loss: 0.6910\n1/ 21, Loss: 0.6880\n...\n50/ 91, Loss: 0.1098\n50/101, Loss: 0.1038\nEpoch 50, Average Test Loss: 0.1048\n\nreal    0m23.212s\nuser    0m21.790s\nsys     0m1.442\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eresults/loss.png\u003c/code\u003e and \u003ccode\u003eresults/loss.csv\u003c/code\u003e were created successfully.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eloss.csv\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etrain_loss,test_loss\n0.6621306538581848,0.6606337598391941\n0.6188064217567444,0.6185613402298519\n0.557918906211853,0.5588848718575069\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eloss.png\u003c/code\u003e: \u003cimg src=\"/notes/loss-plot-1.png\" alt=\"loss plot\"\u003e\u003c/p\u003e\n\u003ch3\u003eExperimentation framework\u003c/h3\u003e\n\u003cp\u003eWe need to optimise the neural net further! I think the loss of ~0.1 we're\ngetting now is still too high. We could bring it down further by tweaking\nhyperparams and perhaps model architecture but let's develop a proper\nframework first to run experiments.\u003c/p\u003e\n\u003cp\u003eI want to be able to do something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-py\"\u003e----------------------------------------------------------------------\nconfig.py:\n\nBATCH_SIZE = Config(default=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, values=[\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e])\nNUM_EPOCHS = Config(default=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, dev=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e)\nLEARNING_RATE = Config(default=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, values=[\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.03\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e])\n----------------------------------------------------------------------\nmain.py:\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erun_training\u003c/span\u003e():\n    train_model()\n\nrun_experiment(variables=[BATCH_SIZE, LEARNING_RATE],\n               function=run_training,\n               time_limit=\u003cspan class=\"hljs-string\"\u003e\"5 mins\"\u003c/span\u003e)\n\n----------------------------------------------------------------------\ntrain.py:\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e config \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e NUM_EPOCHS, LEARNING_RATE, BATCH_SIZE\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etrain_model\u003c/span\u003e():\n    ...\n    optim = SGD(model.parameters(), learning_rate=LEARNING_RATE.get())\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(NUM_EPOCHS.get()):\n        ...\n\n----------------------------------------------------------------------\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat is, I specify all possible values of my hyperparameters and\n\u003ccode\u003erun_experiment\u003c/code\u003e basically tries out all the possible combinations of the\nvariables I specify and stores all the results. So that I could see what the\neffect of each variable is on the training, and also find out the optimal values\nof hyperparameters which are giving me the best loss. I also want it to have a\n\u003ccode\u003edev\u003c/code\u003e mode where it runs experiments on a smaller scale which I can use to test\nmy code before doing a full training run.\u003c/p\u003e\n\u003cp\u003eOkay... implemented this... here's some of the relevant parts of the code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-py\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eExperiment\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, variables=[], dev_mode=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\u003c/span\u003e):\n        self.variables: \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[Config] = variables\n        self.dev_mode: \u003cspan class=\"hljs-built_in\"\u003ebool\u003c/span\u003e = dev_mode\n        ...\n        self.run_number: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n        self.selected_values: \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[Config, \u003cspan class=\"hljs-type\"\u003eAny\u003c/span\u003e] = {}\n        self.all_configs_results: \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, \u003cspan class=\"hljs-type\"\u003eAny\u003c/span\u003e] = {}\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e config \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e ALL_CONFIGS:\n            config.expt = self\n    \n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_config\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, config\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e config \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.selected_values:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.selected_values[config]\n        \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e self.dev_mode:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e config.dev\n        \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e config.default\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erun_experiment\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself,\n                       function=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e,\n                       time_limit: \u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\u003c/span\u003e):\n        value_combinations = itertools.product(\n            *[v.values \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.variables])\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e values \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e value_combinations:\n            self.run_experiment_with_values(function, values)\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erun_experiment_with_values\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, function, values\u003c/span\u003e):\n        self.selected_values = {\n            self.variables[i].name: values[i]\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(self.variables))}\n        \u003cspan class=\"hljs-comment\"\u003e# run_experiment will run this function with selected values of\u003c/span\u003e\n        \u003cspan class=\"hljs-comment\"\u003e# variables\u003c/span\u003e\n        self.run_number += \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e()\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"===================================================\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Experiment run \u003cspan class=\"hljs-subst\"\u003e{self.run_number}\u003c/span\u003e\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"---------------------------------------------------\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Config for this run:\\n\u003cspan class=\"hljs-subst\"\u003e{self.selected_values}\u003c/span\u003e\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"===================================================\"\u003c/span\u003e)\n        start_time = time.time()\n        results = function()\n        end_time = time.time()\n        time_taken = end_time - start_time\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"===================================================\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Run finished in \u003cspan class=\"hljs-subst\"\u003e{time_taken:\u003cspan class=\"hljs-number\"\u003e3.2\u003c/span\u003ef}\u003c/span\u003esec.\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"===================================================\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e()\n        self.all_configs_results.append({\u003cspan class=\"hljs-string\"\u003e'run_num'\u003c/span\u003e: self.run_number})\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e key, value \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.selected_values.items():\n            self.all_configs_results[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][key] = value\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e key, value \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e results.items():\n            self.all_configs_results[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][key] = value\n        self.all_configs_results[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'running_time'\u003c/span\u003e] = time_taken\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e obs \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e ALL_OBSERVERS:\n            obs.set_path(path=self.results_path,\n                         suffix=\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{self.run_number:03d}\u003c/span\u003e\"\u003c/span\u003e)\n            obs.plot()\n            obs.write_csv()\n        self.save_results()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLink to the full code at \u003ca href=\"https://github.com/krkartikay/chess-sl/tree/1547c2a2d19a4b557eb044688467daca2c57c12b\"\u003ecommit \u003ccode\u003e1547c2a\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIt's not really the best code I've written... I'm sure the design isn't as good\nas it could have been... but at least it works.\u003c/p\u003e\n\u003cp\u003eHere's what the results look like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/notes/experiment-results-1.png\" alt=\"experiment results csv and loss graph\"\u003e\u003c/p\u003e\n\u003ch3\u003eAnalysing the results\u003c/h3\u003e\n\u003cp\u003eLoading up the \u003ccode\u003eresults.csv\u003c/code\u003e in excel and applying conditional formatting for\neasier visualisation, here's what I get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/notes/experiment-results-2.png\" alt=\"experiment results spreadsheet\"\u003e\u003c/p\u003e\n\u003cp\u003eWhat're the main conclusions we can draw from this data? The main things I can\nnotice are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLower batch sizes take longer to compute (obvious)\u003c/li\u003e\n\u003cli\u003eSmaller batch sizes train faster (lower loss even with low LR)\u003c/li\u003e\n\u003cli\u003eLarger batch sizes requiring higher LR to converge (gradient competition?)\u003c/li\u003e\n\u003cli\u003eTheoretically I would've guessed lower batch sizes to overfit more easily,\nand it kinda looks like it's going in that direction in run #6 \u0026#x26; #7, train loss\nbecoming slightly lower than test loss... (although I need to run it for\nmore epochs to confirm this...)\u003c/li\u003e\n\u003cli\u003eConversely larger batch sizes should've generalised a bit more (run #20?)\u003c/li\u003e\n\u003cli\u003eFinal loss is around ~0.1-0.09 in all cases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat's it for now. Next time we can try:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImprove neural net architecture: adding conv layers etc.\u003c/li\u003e\n\u003cli\u003eEvaluate the model by making it play games?\u003c/li\u003e\n\u003cli\u003eMore training data\u003c/li\u003e\n\u003cli\u003eVisualisations...\u003c/li\u003e\n\u003c/ul\u003e"},"allNotesIndex":{"base_name":"notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes","notes_data":[{"id":"quotes","metadata":{"title":"Quotes","date":"2023-03-21"}}],"directories":[{"base_name":"book notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes","notes_data":[{"id":"dokkodo","metadata":{"title":"Dokkodo","date":"2023-03-23"}},{"id":"if","metadata":{"title":"\"If\"","date":"2023-03-27"}},{"id":"worry","metadata":{"title":"Worry","date":"2023-03-29"}}],"directories":[{"base_name":"the bhagvad gita","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes/the bhagvad gita","notes_data":[{"id":"chapter_1","metadata":{"title":"Chapter 1","date":"2023-04-06"}},{"id":"chapter_2","metadata":{"title":"Chapter 2","date":"2023-04-08"}},{"id":"index","metadata":{"title":"The Bhagvad Gita","date":"2023-04-06"}}],"directories":[]}]},{"base_name":"economics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics","notes_data":[],"directories":[{"base_name":"microeconomics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics/microeconomics","notes_data":[{"id":"index","metadata":{"title":"Principles of Microeconomics","date":"2023-03-23"}},{"id":"lecture_1","metadata":{"title":"Lecture 1","date":"2023-03-23"}},{"id":"lecture_2","metadata":{"title":"Lecture 2","date":"2023-03-23"}},{"id":"lecture_3","metadata":{"title":"Lecture 3","date":"2023-03-23"}},{"id":"lecture_4","metadata":{"title":"Lecture 4","date":"2023-04-01"}},{"id":"lecture_5","metadata":{"title":"Lecture 5","date":"2023-04-01"}},{"id":"lecture_6","metadata":{"title":"Lecture 6","date":"2023-04-01"}}],"directories":[]}]},{"base_name":"machine learning","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/machine learning","notes_data":[{"id":"cpp-grad","metadata":{"title":"cpp-grad","date":"2023-04-17"}},{"id":"karpathy-lectures","metadata":{"title":"Andrey Karpathy's Lectures","date":"2023-05-14"}},{"id":"study-plan","metadata":{"title":"ML Study plan","date":"2024-01-14"}}],"directories":[{"base_name":"alpha zero","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/machine learning/alpha zero","notes_data":[{"id":"1-the-start","metadata":{"title":"1. The Start","date":"2024-01-14"}},{"id":"2-initial-plan","metadata":{"title":"2. Initial plan","date":"2024-01-14"}},{"id":"3-supervised-learning","metadata":{"title":"3. Supervised Learning","date":"2024-01-15"}},{"id":"4-experimentation","metadata":{"title":"4. Experimentation Framework","date":"2024-01-20"}},{"id":"5-visualization","metadata":{"title":"5. Visualization","date":"2024-01-21"}},{"id":"_notes","metadata":{"title":"_notes"}}],"directories":[]}]}]}},"__N_SSG":true},"page":"/notes/[note_id]","query":{"note_id":"machine learning__alpha zero__4-experimentation"},"buildId":"0cQi27foYwZlvrxIm4tXk","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>