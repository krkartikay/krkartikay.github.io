<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Andrey Karpathy&#x27;s Lectures</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/584705cbbef33b49.css" as="style"/><link rel="stylesheet" href="/_next/static/css/584705cbbef33b49.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-83cebdb887f48834.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb98586f475796ed.js" defer=""></script><script src="/_next/static/chunks/247-fb3f3c148c859656.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bnote_id%5D-774c61eb2a4693cc.js" defer=""></script><script src="/_next/static/R-PaJUMqFitg1dqBVWdzz/_buildManifest.js" defer=""></script><script src="/_next/static/R-PaJUMqFitg1dqBVWdzz/_ssgManifest.js" defer=""></script></head><body class="dark:bg-black dark:bg-opacity-95"><div id="__next"><div><nav class="md:sticky md:top-0 md:z-40 shadow-sm p-2 bg-white dark:bg-black dark:shadow-lg dark:shadow-stone-800"><div class="flex items-center"><a class="flex" href="/"><img alt="Profile pic" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="rounded-full h-20 w-20 mx-2" style="color:transparent" src="/pfp_blank.png"/><h1 class="self-center text-xl text-stone-400 font-mono dark:text-stone-50 dark:drop-shadow-2xl dark:shadow-white dark:hover:text-stone-200">krkartikay&#x27;s notes</h1></a></div></nav><div class="flex flex-row flex-wrap"><aside class="md:w-1/4 text-stone-500 dark:text-stone-200"><div class="max-w-md mx-auto py-8"><a href="/"><h2 class="p-8 text-lg text-stone-600 hover:bg-stone-100 dark:text-stone-200 dark:hover:bg-stone-800 dark:hover:text-stone-200 ">Notes</h2></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Book Notes</li></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__if"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">&quot;If&quot;</li></a><a href="/notes/book%20notes__dokkodo"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Dokkodo</li></a><a href="/notes/book%20notes__the%20bhagvad%20gita__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">The Bhagvad Gita</li></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__the%20bhagvad%20gita__chapter_1"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Chapter 1</li></a><a href="/notes/book%20notes__the%20bhagvad%20gita__chapter_2"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Chapter 2</li></a></ul><a href="/notes/book%20notes__worry"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Worry</li></a></ul><a href="/notes/economics__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Economics</li></a><ul class="list-disc pl-8"><a href="/notes/economics__microeconomics__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Microeconomics</li></a><ul class="list-disc pl-8"><a href="/notes/economics__microeconomics__lecture_1"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 1</li></a><a href="/notes/economics__microeconomics__lecture_2"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 2</li></a><a href="/notes/economics__microeconomics__lecture_3"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 3</li></a><a href="/notes/economics__microeconomics__lecture_4"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 4</li></a><a href="/notes/economics__microeconomics__lecture_5"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 5</li></a><a href="/notes/economics__microeconomics__lecture_6"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Lecture 6</li></a></ul></ul><a href="/notes/machine%20learning__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Machine Learning</li></a><ul class="list-disc pl-8"><a href="/notes/machine%20learning__alpha%20zero__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Alpha Zero</li></a><ul class="list-disc pl-8"><a href="/notes/machine%20learning__alpha%20zero__1-the-start"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">1. The Start</li></a><a href="/notes/machine%20learning__alpha%20zero__2-initial-plan"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">2. Initial plan</li></a><a href="/notes/machine%20learning__alpha%20zero__3-supervised-learning"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">3. Supervised Learning</li></a><a href="/notes/machine%20learning__alpha%20zero___notes"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">_notes</li></a></ul><a href="/notes/machine%20learning__karpathy-lectures"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside font-bold bg-stone-50 dark:bg-stone-800">Andrey Karpathy&#x27;s Lectures</li></a><a href="/notes/machine%20learning__study-plan"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">ML Study plan</li></a><a href="/notes/machine%20learning__cpp-grad"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">cpp-grad</li></a></ul><a href="/notes/quotes"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 dark:text-stone-400 dark:hover:bg-stone-800 dark:hover:text-stone-200 list-inside ">Quotes</li></a></ul></div></aside><main class="md:w-3/4 border-l-2 dark:border-l-stone-900 border-l-stone-50"><div class="p-8 md:p-16"><br/><h1 class="text-4xl dark:text-white">Andrey Karpathy&#x27;s Lectures</h1><p class="text-stone-400 dark:text-stone-300">2023-05-14</p><div class="prose prose-stone dark:prose-invert"><div><p>Just finished watching Andrej Karpathy's Neural Networks: Zero to Hero series on Youtube.</p>
<p>First, before anything else, I'd like to say that Andrej Karpathy is an amazing person for uploading these lectures for all of us even though he must have a very busy life. I agree with him that AI can bring prosperity for all only if everyone understands AI and learn how to use it. I believe that power concentrated in the hands of a few will not be good for humanity and the best we can do right now is to try and distribute this power by learning ML, training our own models and doing more <em>Open</em> research on AI. Andrej has really upheld his <em>dharma</em> as a teacher by contributing this lecture series for all of us.</p>
<p>Having said that, just writing my own thoughts on the lectures:</p>
<ol>
<li>
<p><strong>First lecture, Micrograd.</strong> Interesting refresher on how backpropagation works along with practical implementation of how to write an autograd engine. Was probably worth watching, but probably not worth implementing (for me at least); since I think I already understood how backpropagation (in the autograd sense) works. I had "figured it out" in college one time. May have been tremendously useful for other people if they didn't understand backprop yet.</p>
</li>
<li>
<p><strong>Second lecture, Bigram model and a basic neural net.</strong> Was probably worth watching to see how Andrej coded it and trained it. Intersting insights into his workflow.</p>
</li>
<li>
<p><strong>Third + Fourth lecture, MLP multi-char context model.</strong> The model I already knew how to build, but <em>the way Andrej discussed how to train it, how to stabilise it with batchnorm, the initialisation and visualisations during backpropagation, all that was very valuable information</em>. (Especially all the info in the fourth lecture, that was completely new for me). Training neural nets is basically guesswork unless you know how to do this, and guesswork is how I used to do it earlier. No wonder I had given up on neural net related projects last time when I was trying to implement RL algorithms myself.</p>
</li>
<li>
<p><strong>Fifth lecture, Backpropagating manually.</strong> We had been taught how to backprop manually in college, so while this lecture was a good exercise for the brain, but I don't think it was very valuable for me personally. Could have saved hours I spent on this. I'm not going to backprop through a batchnorm layer again in my life anyway! (so I hope... lol).</p>
</li>
<li>
<p><strong>Sixth lecture, Wavenet.</strong> I'm not sure if I gained anything new from this lecture at all. Probably I may have missed something important (if there was anything at all) because I watched this lecture half-falling-asleep, but still I think doing RNN's instead of this could have been better.</p>
</li>
<li>
<p><strong>Seventh, last lecture, NanoGPT.</strong> So basically Andrej walked us through implementing NanoGPT, starting from the bigram model. Up to the halfway mark, there's really nothing 'new'. So if you, like me, just wanted to understand how transformers work, you could skip to almost half-way into this lecture and get started from there.</p>
</li>
</ol>
<p>He explained the attention mechanism in a really intuitive way, and since I am already familiar with the matrix math, I found it really easy to follow, but the way he explained it would have been easy enough to follow even if you haven't worked with matrices much (yet).</p>
<p>I had expected Andrej to explain a bit more about how to set up GPU training etc. but I think that was not interesting for him and he just skipped over all that with one line:</p>
<p>"...and I scaled up and trained this neural network which took about 15 minutes on my A100 GPU. I would not recommend training this network on a CPU or a macbook. You would have to reduce the number of layers, n_embd, and so on..."</p>
<p>However he did mention in the video description that the GPU he's using is from lambdalabs, again very valuable information.</p>
<blockquote>
<p>The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: <a href="https://lambdalabs.com">https://lambdalabs.com</a> . If you prefer to work in notebooks, I think the easiest path today is Google Colab.</p>
</blockquote>
<p>An open question still remains in my mind though: <em>Was the A100 really required?</em> How would the training perform on a 1080Ti for example? Or on an Nvidia 3060 GPU? Would it take 15 minutes still, to achieve the same loss? or maybe 30 minutes, or 24 hours? If it does run on a normal GPU in a reasonable amount of time, then how would it do on a macbook with ANE acceleration turned on? What about inference??</p>
<p>Overall I think this whole series was really valuable (for the insights into his thinking and workflow), and I wish Andrej (and other people in the industry) would do more videos of this type.</p>
<p>However, what I found severely missing from this series, which I had hoped it would contain, was the evolution of RNNs and LSTMs and how we ultimately got to Transformers. Because that would show <em>all the ways that don't work</em>, not just the one thing that does work. (Which is, in my opinion, way more valuable). I think Andrej rushed to complete this lecture series with this one lecture, (he probably found something more important to do with his time at OpenAI?), because he did mention at the start that he would probably cover RNNs some day.</p>
<p>I would have to study that from somewhere else now, but anyways I'm thankful to even have gotten to study this much from Andrej.</p>
<hr>
<p>Now, I realise, the hard part actually begins. Really understanding all of this, maybe implementing all this on my own, reading the research papers, experimenting, etc. etc...</p></div></div></div></main></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note_id":"machine learning__karpathy-lectures","noteData":{"metadata":{"title":"Andrey Karpathy's Lectures","date":"2023-05-14"},"content":"\u003cp\u003eJust finished watching Andrej Karpathy's Neural Networks: Zero to Hero series on Youtube.\u003c/p\u003e\n\u003cp\u003eFirst, before anything else, I'd like to say that Andrej Karpathy is an amazing person for uploading these lectures for all of us even though he must have a very busy life. I agree with him that AI can bring prosperity for all only if everyone understands AI and learn how to use it. I believe that power concentrated in the hands of a few will not be good for humanity and the best we can do right now is to try and distribute this power by learning ML, training our own models and doing more \u003cem\u003eOpen\u003c/em\u003e research on AI. Andrej has really upheld his \u003cem\u003edharma\u003c/em\u003e as a teacher by contributing this lecture series for all of us.\u003c/p\u003e\n\u003cp\u003eHaving said that, just writing my own thoughts on the lectures:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFirst lecture, Micrograd.\u003c/strong\u003e Interesting refresher on how backpropagation works along with practical implementation of how to write an autograd engine. Was probably worth watching, but probably not worth implementing (for me at least); since I think I already understood how backpropagation (in the autograd sense) works. I had \"figured it out\" in college one time. May have been tremendously useful for other people if they didn't understand backprop yet.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSecond lecture, Bigram model and a basic neural net.\u003c/strong\u003e Was probably worth watching to see how Andrej coded it and trained it. Intersting insights into his workflow.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eThird + Fourth lecture, MLP multi-char context model.\u003c/strong\u003e The model I already knew how to build, but \u003cem\u003ethe way Andrej discussed how to train it, how to stabilise it with batchnorm, the initialisation and visualisations during backpropagation, all that was very valuable information\u003c/em\u003e. (Especially all the info in the fourth lecture, that was completely new for me). Training neural nets is basically guesswork unless you know how to do this, and guesswork is how I used to do it earlier. No wonder I had given up on neural net related projects last time when I was trying to implement RL algorithms myself.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFifth lecture, Backpropagating manually.\u003c/strong\u003e We had been taught how to backprop manually in college, so while this lecture was a good exercise for the brain, but I don't think it was very valuable for me personally. Could have saved hours I spent on this. I'm not going to backprop through a batchnorm layer again in my life anyway! (so I hope... lol).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSixth lecture, Wavenet.\u003c/strong\u003e I'm not sure if I gained anything new from this lecture at all. Probably I may have missed something important (if there was anything at all) because I watched this lecture half-falling-asleep, but still I think doing RNN's instead of this could have been better.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSeventh, last lecture, NanoGPT.\u003c/strong\u003e So basically Andrej walked us through implementing NanoGPT, starting from the bigram model. Up to the halfway mark, there's really nothing 'new'. So if you, like me, just wanted to understand how transformers work, you could skip to almost half-way into this lecture and get started from there.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHe explained the attention mechanism in a really intuitive way, and since I am already familiar with the matrix math, I found it really easy to follow, but the way he explained it would have been easy enough to follow even if you haven't worked with matrices much (yet).\u003c/p\u003e\n\u003cp\u003eI had expected Andrej to explain a bit more about how to set up GPU training etc. but I think that was not interesting for him and he just skipped over all that with one line:\u003c/p\u003e\n\u003cp\u003e\"...and I scaled up and trained this neural network which took about 15 minutes on my A100 GPU. I would not recommend training this network on a CPU or a macbook. You would have to reduce the number of layers, n_embd, and so on...\"\u003c/p\u003e\n\u003cp\u003eHowever he did mention in the video description that the GPU he's using is from lambdalabs, again very valuable information.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: \u003ca href=\"https://lambdalabs.com\"\u003ehttps://lambdalabs.com\u003c/a\u003e . If you prefer to work in notebooks, I think the easiest path today is Google Colab.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAn open question still remains in my mind though: \u003cem\u003eWas the A100 really required?\u003c/em\u003e How would the training perform on a 1080Ti for example? Or on an Nvidia 3060 GPU? Would it take 15 minutes still, to achieve the same loss? or maybe 30 minutes, or 24 hours? If it does run on a normal GPU in a reasonable amount of time, then how would it do on a macbook with ANE acceleration turned on? What about inference??\u003c/p\u003e\n\u003cp\u003eOverall I think this whole series was really valuable (for the insights into his thinking and workflow), and I wish Andrej (and other people in the industry) would do more videos of this type.\u003c/p\u003e\n\u003cp\u003eHowever, what I found severely missing from this series, which I had hoped it would contain, was the evolution of RNNs and LSTMs and how we ultimately got to Transformers. Because that would show \u003cem\u003eall the ways that don't work\u003c/em\u003e, not just the one thing that does work. (Which is, in my opinion, way more valuable). I think Andrej rushed to complete this lecture series with this one lecture, (he probably found something more important to do with his time at OpenAI?), because he did mention at the start that he would probably cover RNNs some day.\u003c/p\u003e\n\u003cp\u003eI would have to study that from somewhere else now, but anyways I'm thankful to even have gotten to study this much from Andrej.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eNow, I realise, the hard part actually begins. Really understanding all of this, maybe implementing all this on my own, reading the research papers, experimenting, etc. etc...\u003c/p\u003e"},"allNotesIndex":{"base_name":"notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes","notes_data":[{"id":"quotes","metadata":{"title":"Quotes","date":"2023-03-21"}}],"directories":[{"base_name":"book notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes","notes_data":[{"id":"dokkodo","metadata":{"title":"Dokkodo","date":"2023-03-23"}},{"id":"if","metadata":{"title":"\"If\"","date":"2023-03-27"}},{"id":"worry","metadata":{"title":"Worry","date":"2023-03-29"}}],"directories":[{"base_name":"the bhagvad gita","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes/the bhagvad gita","notes_data":[{"id":"chapter_1","metadata":{"title":"Chapter 1","date":"2023-04-06"}},{"id":"chapter_2","metadata":{"title":"Chapter 2","date":"2023-04-08"}},{"id":"index","metadata":{"title":"The Bhagvad Gita","date":"2023-04-06"}}],"directories":[]}]},{"base_name":"economics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics","notes_data":[],"directories":[{"base_name":"microeconomics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics/microeconomics","notes_data":[{"id":"index","metadata":{"title":"Principles of Microeconomics","date":"2023-03-23"}},{"id":"lecture_1","metadata":{"title":"Lecture 1","date":"2023-03-23"}},{"id":"lecture_2","metadata":{"title":"Lecture 2","date":"2023-03-23"}},{"id":"lecture_3","metadata":{"title":"Lecture 3","date":"2023-03-23"}},{"id":"lecture_4","metadata":{"title":"Lecture 4","date":"2023-04-01"}},{"id":"lecture_5","metadata":{"title":"Lecture 5","date":"2023-04-01"}},{"id":"lecture_6","metadata":{"title":"Lecture 6","date":"2023-04-01"}}],"directories":[]}]},{"base_name":"machine learning","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/machine learning","notes_data":[{"id":"cpp-grad","metadata":{"title":"cpp-grad","date":"2023-04-17"}},{"id":"karpathy-lectures","metadata":{"title":"Andrey Karpathy's Lectures","date":"2023-05-14"}},{"id":"study-plan","metadata":{"title":"ML Study plan","date":"2024-01-14"}}],"directories":[{"base_name":"alpha zero","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/machine learning/alpha zero","notes_data":[{"id":"1-the-start","metadata":{"title":"1. The Start","date":"2024-01-14"}},{"id":"2-initial-plan","metadata":{"title":"2. Initial plan","date":"2024-01-14"}},{"id":"3-supervised-learning","metadata":{"title":"3. Supervised Learning","date":"2024-01-15"}},{"id":"_notes","metadata":{"title":"_notes"}}],"directories":[]}]}]}},"__N_SSG":true},"page":"/notes/[note_id]","query":{"note_id":"machine learning__karpathy-lectures"},"buildId":"R-PaJUMqFitg1dqBVWdzz","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>