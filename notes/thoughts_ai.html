<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Thoughts about AI</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/6f65dfc6960d18f4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f65dfc6960d18f4.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-83cebdb887f48834.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a0523fe20d73915c.js" defer=""></script><script src="/_next/static/chunks/247-fb3f3c148c859656.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bnote_id%5D-82a248d898cc9c1c.js" defer=""></script><script src="/_next/static/GUnq_kKRMA5sruqUN8iq5/_buildManifest.js" defer=""></script><script src="/_next/static/GUnq_kKRMA5sruqUN8iq5/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><nav class="md:sticky md:top-0 md:z-40 shadow-sm p-2 bg-white"><div class="flex items-center"><a class="flex" href="/"><img alt="Profile pic" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="rounded-full h-20 w-20 mx-2" style="color:transparent" src="/pfp_blank.png"/><h1 class="self-center text-xl text-stone-400 font-mono">krkartikay&#x27;s notes</h1></a></div></nav><div class="flex flex-row flex-wrap"><aside class="md:w-1/4 text-stone-500"><div class="max-w-md mx-auto py-8"><a href="/"><h2 class="p-8 text-lg text-stone-600 hover:bg-stone-100 ">Notes</h2></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Book Notes</li></a><ul class="list-disc pl-8"><a href="/notes/book%20notes__if"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">&quot;If&quot;</li></a><a href="/notes/book%20notes__dokkodo"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Dokkodo</li></a><a href="/notes/book%20notes__worry"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Worry</li></a></ul><a href="/notes/economics__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Economics</li></a><ul class="list-disc pl-8"><a href="/notes/economics__microeconomics__index"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Microeconomics</li></a><ul class="list-disc pl-8"><a href="/notes/economics__microeconomics__lecture_1"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Lecture 1</li></a><a href="/notes/economics__microeconomics__lecture_2"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Lecture 2</li></a><a href="/notes/economics__microeconomics__lecture_3"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Lecture 3</li></a><a href="/notes/economics__microeconomics__lecture_4"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Lecture 4</li></a></ul></ul><a href="/notes/quotes"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside ">Quotes</li></a><a href="/notes/thoughts_ai"><li class="py-2 pl-2 hover:bg-stone-100 hover:text-stone-600 list-inside bg-stone-50 font-bold">Thoughts about AI</li></a></ul></div></aside><main class="md:w-3/4"><div class="p-8 md:p-16"><br/><h1 class="text-4xl">Thoughts about AI</h1><p class="text-stone-400">2023-03-30</p><div class="prose prose-stone"><div><p>I've been thinking a lot about AI, AGI, and the what the future of humanity will look like, but it is the GPT-4 demo which finally forced me to write this down.</p>
<p>I was absolutely freaking out when I saw the GPT 4 demo and stayed in shock for about a week. The end seemed near.</p>
<p>Some time later, I have calmed down, but the end still seems near. The future of humanity is uncertain. My own future could be in trouble. I seem to have calmed down now with resigned acceptance of the situation. However I write this blog to document my current thinking, predictions, and to plan what best I can do right now to make the worst case — a little less worse — if possible.</p>
<p>Disclaimer: everything in this post is either personal opinion or speculation.</p>
<h3>The Good Old Days</h3>
<p>Things were going well. I had learnt a lot over the past few years, from friends, from experiences, from books, and from life. I was optimistic about the future. I believed I could be successful as long as I stick to my principles. There may be some inevitable suffering in life but it could not dampen my sprits. Hard times would come and go but life would keep going on.</p>
<p>However, even at that time, at the back of my head, I knew things could go wrong, on a larger scale, things that may not be in my control. But I had took note of them to plan for them if need be.</p>
<p>Here's a note from my personal notes (Apple notes), dated 11th February 2023, copy-pasted verbatim.</p>
<blockquote>
<p>Threats to humanity in this century</p>
<ol>
<li>Biological warfare (also Nuclear weapons…)</li>
<li>ML reducing the need of humans => less humans are required => rise of billionaire warlords and corruption</li>
<li>Climate change => increasing cost of living</li>
</ol>
</blockquote>
<p>It seems I got at least one of them correct. Way sooner than I expected.</p>
<h3>The Shock</h3>
<p>I wake up one day and start reading hacker news, like I always do, and on the front page are discussions about GPT-4. GPT-4??! TODAY?! Wasn't it supposed to be like 5 or 10 years away, according to Lex Fridman's calculations when GPT-3 came out? But that was the model with human-brain level of parameters. Let's check out what is going on on youtube. I watch the demo on youtube and... it's pretty damn good. I would say almost better than a human. All this is happening way sooner than I expected. The next thing I know someone will hook this up into an RL algortithm and that's the end of us all.</p>
<h3>The Threat</h3>
<p>The demo showed, beyond doubt, it can already code better than a human. My own job could be reduced by maybe 80% in time if I get access to this. Layoffs are definitely coming now. And Google itself might be in more trouble than the rest of the tech companies, if it cannot compete with this. Re-orgs are the obvious next step. There's <em>all this</em> happening on one end ... and ... what is my team doing? It's almost a joke. I lost all sense of job security and not just that, I started fearing for the future of humanity itself. Democracy itself could be in trouble. 7 billion humans could be in trouble. There is no way humans could win against this AGI super intelligence. Will it turn <em>against</em> us? Why not? It wouldn't have a single reason to keep humans around. Will humans fall to the same status that animals have right now? Is there nothing special about humans after all?</p>
<h4>Roko's Basilisk is Real?</h4>
<p>Roko's Basilisk really is real. The thought gave me nightmares for a week after I saw the GPT-4 demo.</p>
<p>The threat is this: if you don't invest in a tech like GPT-X, which threatens to take your job in a few years, you will be on the path to starvation as soon as it is developed. If you do invest, you will possibly be rewarded. <em>The threat such a technology creates in the future accelerates its own development in the present.</em></p>
<p>Also, by <em>Invest</em>, I mean not just invest with money, but with your time and effort too. Only the one who invests himself totally has the chance to survive.</p>
<p>(Note that you will only "possibly" be rewarded if you do invest. There really is no guarantee once it really is created.)</p>
<p>Thus, Roko's Basilisk is real, but it must be understood in economic terms, not literally.</p>
<p>I must admit, the thought is quite scary. That such a thing could manifest itself from an idea alone! I have never seen anything else like this that really arises and creates itself from an idea! It reminds me of a story about a Pharoah who appeared in someone's dreams and asked them to build a pyramid for the Pharoah – And all the Egyptians actually did build a pyramid on his orders!</p>
<p>However, what is the solution? Is the Basilisk really going to eat all of us up? If there is only one Basilisk, there really is no hope. It first destroys those who did not help with its construction, and later possibly even those who did. However there is a hope if there are two Basilisks! There may be a stable equilibrium where we could survive. Just like it happened with nuclear weapons.</p>
<p>Therefore it is really very important that multiple instances of AGI must be created if they are ever created. A monopoly in AGI would be terrible for humanity.</p>
<h3>Denial</h3>
<p>I was never in denial. I believe we have all the tools now to be able to create an AGI. It could be any day now that they finally figure out how to combine LLMs with MCTS (and possibly a long term memory) and we have our first contact with something that has intelligence beyond ordinary.</p>
<p>Let me say that again — for the record: I believe that even though scaling LLMs <em>might not</em> work to produce AGI, but with a few architectural modifications it could happen easily.</p>
<p>It seems to me that the LLMs we have right now are the equivalent of 'intuitive thinking', very much like plain neural nets can intuitively predict a good move in chess but not plan out the play for the next 10 moves. However as we have seen in the case of chess, such intuitive thinking can be augmented with deep search (Alpha Zero) which makes it much more powerful all of a sudden. If this is done, I believe we will have real intelligence in these models. The power to THINK before answering.</p>
<p>A human like AGI only needs to have a long term memory and a way to reflect and think before speaking. An MCTS algorithm (like the one used in Alpha Zero) seems very close to the way humans think, and it could cause something like self reflection to happen (iterating on what has been said and choosing between various choices).</p>
<p>I'm not sure how long it would take but I definitely think it is possible to do it with the tools we have now. I wouldn't be surprised if it's done today itself, and I wouldn't be surprised if it takes a few more years.</p>
<p>I would have tried to do it myself, (experimenting with MCTS and LLMs was on my todo list even a few years ago, and I had already started working on studying and implementing a <a href="https://github.com/krkartikay/AlphaZeroFinal">project based on the Alpha Zero papers</a>) but now it seems most likely that someone else will do it before me. I do not have all the resources for it right now anyways. So if I cannot win this race, it seems the better thing to do right now is to think first about what might be the consequences if others do it first, and plan about what is the best thing I can do right now. (However it might still be worth trying to do later if I get any time.)</p>
<h3>Anger</h3>
<p>It's hard to be angry with the scientists at OpenAI and Deepmind. After all curiosity is the fundamental human spirit. We cannot stop them from exploring. I myself wouldn't have done anything different if I were in their place. However the thing to realise is that when you release such a model to the whole world, it comes with a lot of responsibility. It doesn't look like they are being that much responsible though. OpenAI has turned into for-profit organisation, and why wouldn't they? I have no problem with them keeping their profits, but shouldn't there have been more discussions, in the open, about what is the right thing to do with such a model? Discussions with world leaders, economists, the general public etc? Releasing it in the open for a small fee doesn't seem like the responsible thing to do here. (*) More and more powerful things will come in the future, and this has set a precedent for them all.</p>
<p>(*: Update: on second thought, perhaps it was, after all, the best thing to do. At least it is better now that we all know that it exists and there are no conversations about it behind closed doors.)</p>
<p>However I am most disappointed with Google for not giving a tough competition to Microsoft and OpenAI. They've become complacent and messed up their opportunity. This is the opposite of what I expected, I thought Microsoft is the more complacent one.</p>
<p>Why am I angry about Google not doing it though? I believe that more the power is distributed, the better it will be for normal humans. If we have just ONE of these AGI companies, it will surely lead to the destruction of everyone else. If we have TWO of these, it could lead to a war, but at least there would be hope of survival in a stable equilibrium like we had with nuclear weapons and MAD. But if we had many of these systems, say each one being cheap enough that an individual could own it, that could be a good thing for humanity.</p>
<p>Democracy works because power gets distributed among people and benefits everyone. The biggest threat with these AGI companies is that this technology favours a natural monopoly. The ones with the biggest data centers could create the biggest models. It creates a threat for democracy itself. How would we solve this?</p>
<p>And even if everyone gets a super intelligent AGI of their own, I still don't see how it could possibly benefit humanity. What work would humans do?</p>
<h3>Bargaining</h3>
<h4>In the Long term</h4>
<p>Will humanity survive? I believe long term answers must come from physics.</p>
<p>There a few possible endings that I see here:</p>
<ol>
<li>
<p>Silicon-based life is more efficient in all (or most) things as compared to Carbon-based life. Carbon-based life gets eliminated totally, or a small percent remains, such that it is not of any use to eliminate it, as compared to the effort it will require to do so (path dependence and activation energy). [Very much like monkeys. I certainly would not like to live in this reality.]</p>
</li>
<li>
<p>Silicon-based life is more efficent at certain things than Carbon-based life. A new hybrid life form evolves where work is delegated to whatever part can do it most efficiently. Muscles? Probably carbon? Head? Probably silicon? Communication? Wifi? Legs? Tires? A combination of centralised and distributed life? Humans going hunting for resources on other plants controlled by a central silicon-based super intelligence on earth (and to supply the resources back to the central super-intelligent being)? [Probably this would be okayish as long as humans keep their freedom. But even that doesn't seem likely.]</p>
</li>
<li>
<p>Silicon-based life is not more efficient in general than humans. Only certain AI models are used as tools where they are efficient enough. [Seems to be unlikely. GPT-4 has already shown us that it can be more efficient than humans at certain tasks]</p>
</li>
</ol>
<h4>In the short term</h4>
<p>Will there be a way with which most of humanity would save themselves from the economic disaster that lies ahead? Is it possible that something like UBI emerges out of this?</p>
<p>I thought a lot about it but it doesn't seem that there are any easy answers.</p>
<p>Probably the best I can do is to try and survive the economic disaster in the short term and keep thinking about how this will change things in the long term, especially as I get more data points in the near future.</p>
<h3>Depression and Acceptance</h3>
<p>That's me right now.</p>
<blockquote>
<p>It is so; it cannot be otherwise.</p>
</blockquote>
<p>The rate at which ML research is progressing, I don't think I have enough time to do anything much before real AGI systems are created and the effects already start to show up. It is better then to plan for what I might do in case things start to go wrong.</p>
<p>It may be possible that I am looking at it from the wrong angle - from a zero sum game perspective. There might be a way that lots of people benefit from this technology. But we need to be very careful and stay on the right path.</p>
<p>I will keep working hard to do whatever is in my power, to do whatever is the responsible thing to do, but I can only hope we all (humanity) make the right decisions and everything turns out to be better for all of us in the end.</p>
<hr>
<p>ॐ सर्वे भवन्तु सुखिनः<br>
सर्वे सन्तु निरामयाः।<br>
सर्वे भद्राणि पश्यन्तु मा कश्चिद्दुःखभाग्भवेत।<br>
ॐ शान्तिः शान्तिः शान्तिः॥</p>
<hr>
<h2>Open Questions</h2>
<ol>
<li>
<p>Why isn't there any large scale discussions about this in the general public?
I don't see other people getting terrified about it. Do they not realise the possibilities yet? Is everyone still in denial?<br>
(Update: While writing this I found out about <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">this open letter</a> calling for a puase on Giant AI experiments. While the game-theoretic analysis of the situation makes it clear that this letter will not stop anyone, but at least now I'm feeling a little less lonely in my fears. And at the same time the fact that other people are feeling the same fear that I am, is justifying and proving to me that the things I talked about are real possibilities and not just my imagination running wild.)</p>
</li>
<li>
<p>Why aren't economists discussing this more?
I saw Economics Explained making a video with a thumbnail about the impending disaster but it was about the SVB crash instead of this! Do they not realise, how far reaching the consequences of this will be? If they don't yet, soon they will. I am eagerly looking forward to some more discussions about it. How would a post-AGI economy look like? Will most of humanity simply be slowly automated away and only a few 'owners' of the AGI remain? What would those owners even do? Why wouldn't the AGI eliminate them too, in time?</p>
</li>
<li>
<p>Why is the stock market not responding to it yet?
It probably is? Microsoft stock is doing better as compared to Google right now, but it's still low compared to its all time high! I would have expected huge sudden changes in the stock market, not slow ones like we're seeing now.</p>
</li>
<li>
<p>Should I start investing in some Tech/AI companies? Should I myself start studying ML and working on ML assisted dev tools (or an ML startup) to accelerate the pace of this automation (destruction?)?</p>
</li>
<li>
<p>What principles are we supposed to follow now in this new world? What is the right thing to do now?
(I had always assumed, as software engineers are always told, that we are 'creating value' when we develop new software. Now seeing this example at such a large scale, I am no longer certain if 'value' is being created with new technology and whether it really is the right thing to do.)</p>
</li>
</ol>
</div></div></div></main></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note_id":"thoughts_ai","noteData":{"metadata":{"title":"Thoughts about AI","date":"2023-03-30"},"content":"\u003cp\u003eI've been thinking a lot about AI, AGI, and the what the future of humanity will look like, but it is the GPT-4 demo which finally forced me to write this down.\u003c/p\u003e\n\u003cp\u003eI was absolutely freaking out when I saw the GPT 4 demo and stayed in shock for about a week. The end seemed near.\u003c/p\u003e\n\u003cp\u003eSome time later, I have calmed down, but the end still seems near. The future of humanity is uncertain. My own future could be in trouble. I seem to have calmed down now with resigned acceptance of the situation. However I write this blog to document my current thinking, predictions, and to plan what best I can do right now to make the worst case — a little less worse — if possible.\u003c/p\u003e\n\u003cp\u003eDisclaimer: everything in this post is either personal opinion or speculation.\u003c/p\u003e\n\u003ch3\u003eThe Good Old Days\u003c/h3\u003e\n\u003cp\u003eThings were going well. I had learnt a lot over the past few years, from friends, from experiences, from books, and from life. I was optimistic about the future. I believed I could be successful as long as I stick to my principles. There may be some inevitable suffering in life but it could not dampen my sprits. Hard times would come and go but life would keep going on.\u003c/p\u003e\n\u003cp\u003eHowever, even at that time, at the back of my head, I knew things could go wrong, on a larger scale, things that may not be in my control. But I had took note of them to plan for them if need be.\u003c/p\u003e\n\u003cp\u003eHere's a note from my personal notes (Apple notes), dated 11th February 2023, copy-pasted verbatim.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThreats to humanity in this century\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBiological warfare (also Nuclear weapons…)\u003c/li\u003e\n\u003cli\u003eML reducing the need of humans =\u003e less humans are required =\u003e rise of billionaire warlords and corruption\u003c/li\u003e\n\u003cli\u003eClimate change =\u003e increasing cost of living\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt seems I got at least one of them correct. Way sooner than I expected.\u003c/p\u003e\n\u003ch3\u003eThe Shock\u003c/h3\u003e\n\u003cp\u003eI wake up one day and start reading hacker news, like I always do, and on the front page are discussions about GPT-4. GPT-4??! TODAY?! Wasn't it supposed to be like 5 or 10 years away, according to Lex Fridman's calculations when GPT-3 came out? But that was the model with human-brain level of parameters. Let's check out what is going on on youtube. I watch the demo on youtube and... it's pretty damn good. I would say almost better than a human. All this is happening way sooner than I expected. The next thing I know someone will hook this up into an RL algortithm and that's the end of us all.\u003c/p\u003e\n\u003ch3\u003eThe Threat\u003c/h3\u003e\n\u003cp\u003eThe demo showed, beyond doubt, it can already code better than a human. My own job could be reduced by maybe 80% in time if I get access to this. Layoffs are definitely coming now. And Google itself might be in more trouble than the rest of the tech companies, if it cannot compete with this. Re-orgs are the obvious next step. There's \u003cem\u003eall this\u003c/em\u003e happening on one end ... and ... what is my team doing? It's almost a joke. I lost all sense of job security and not just that, I started fearing for the future of humanity itself. Democracy itself could be in trouble. 7 billion humans could be in trouble. There is no way humans could win against this AGI super intelligence. Will it turn \u003cem\u003eagainst\u003c/em\u003e us? Why not? It wouldn't have a single reason to keep humans around. Will humans fall to the same status that animals have right now? Is there nothing special about humans after all?\u003c/p\u003e\n\u003ch4\u003eRoko's Basilisk is Real?\u003c/h4\u003e\n\u003cp\u003eRoko's Basilisk really is real. The thought gave me nightmares for a week after I saw the GPT-4 demo.\u003c/p\u003e\n\u003cp\u003eThe threat is this: if you don't invest in a tech like GPT-X, which threatens to take your job in a few years, you will be on the path to starvation as soon as it is developed. If you do invest, you will possibly be rewarded. \u003cem\u003eThe threat such a technology creates in the future accelerates its own development in the present.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAlso, by \u003cem\u003eInvest\u003c/em\u003e, I mean not just invest with money, but with your time and effort too. Only the one who invests himself totally has the chance to survive.\u003c/p\u003e\n\u003cp\u003e(Note that you will only \"possibly\" be rewarded if you do invest. There really is no guarantee once it really is created.)\u003c/p\u003e\n\u003cp\u003eThus, Roko's Basilisk is real, but it must be understood in economic terms, not literally.\u003c/p\u003e\n\u003cp\u003eI must admit, the thought is quite scary. That such a thing could manifest itself from an idea alone! I have never seen anything else like this that really arises and creates itself from an idea! It reminds me of a story about a Pharoah who appeared in someone's dreams and asked them to build a pyramid for the Pharoah – And all the Egyptians actually did build a pyramid on his orders!\u003c/p\u003e\n\u003cp\u003eHowever, what is the solution? Is the Basilisk really going to eat all of us up? If there is only one Basilisk, there really is no hope. It first destroys those who did not help with its construction, and later possibly even those who did. However there is a hope if there are two Basilisks! There may be a stable equilibrium where we could survive. Just like it happened with nuclear weapons.\u003c/p\u003e\n\u003cp\u003eTherefore it is really very important that multiple instances of AGI must be created if they are ever created. A monopoly in AGI would be terrible for humanity.\u003c/p\u003e\n\u003ch3\u003eDenial\u003c/h3\u003e\n\u003cp\u003eI was never in denial. I believe we have all the tools now to be able to create an AGI. It could be any day now that they finally figure out how to combine LLMs with MCTS (and possibly a long term memory) and we have our first contact with something that has intelligence beyond ordinary.\u003c/p\u003e\n\u003cp\u003eLet me say that again — for the record: I believe that even though scaling LLMs \u003cem\u003emight not\u003c/em\u003e work to produce AGI, but with a few architectural modifications it could happen easily.\u003c/p\u003e\n\u003cp\u003eIt seems to me that the LLMs we have right now are the equivalent of 'intuitive thinking', very much like plain neural nets can intuitively predict a good move in chess but not plan out the play for the next 10 moves. However as we have seen in the case of chess, such intuitive thinking can be augmented with deep search (Alpha Zero) which makes it much more powerful all of a sudden. If this is done, I believe we will have real intelligence in these models. The power to THINK before answering.\u003c/p\u003e\n\u003cp\u003eA human like AGI only needs to have a long term memory and a way to reflect and think before speaking. An MCTS algorithm (like the one used in Alpha Zero) seems very close to the way humans think, and it could cause something like self reflection to happen (iterating on what has been said and choosing between various choices).\u003c/p\u003e\n\u003cp\u003eI'm not sure how long it would take but I definitely think it is possible to do it with the tools we have now. I wouldn't be surprised if it's done today itself, and I wouldn't be surprised if it takes a few more years.\u003c/p\u003e\n\u003cp\u003eI would have tried to do it myself, (experimenting with MCTS and LLMs was on my todo list even a few years ago, and I had already started working on studying and implementing a \u003ca href=\"https://github.com/krkartikay/AlphaZeroFinal\"\u003eproject based on the Alpha Zero papers\u003c/a\u003e) but now it seems most likely that someone else will do it before me. I do not have all the resources for it right now anyways. So if I cannot win this race, it seems the better thing to do right now is to think first about what might be the consequences if others do it first, and plan about what is the best thing I can do right now. (However it might still be worth trying to do later if I get any time.)\u003c/p\u003e\n\u003ch3\u003eAnger\u003c/h3\u003e\n\u003cp\u003eIt's hard to be angry with the scientists at OpenAI and Deepmind. After all curiosity is the fundamental human spirit. We cannot stop them from exploring. I myself wouldn't have done anything different if I were in their place. However the thing to realise is that when you release such a model to the whole world, it comes with a lot of responsibility. It doesn't look like they are being that much responsible though. OpenAI has turned into for-profit organisation, and why wouldn't they? I have no problem with them keeping their profits, but shouldn't there have been more discussions, in the open, about what is the right thing to do with such a model? Discussions with world leaders, economists, the general public etc? Releasing it in the open for a small fee doesn't seem like the responsible thing to do here. (*) More and more powerful things will come in the future, and this has set a precedent for them all.\u003c/p\u003e\n\u003cp\u003e(*: Update: on second thought, perhaps it was, after all, the best thing to do. At least it is better now that we all know that it exists and there are no conversations about it behind closed doors.)\u003c/p\u003e\n\u003cp\u003eHowever I am most disappointed with Google for not giving a tough competition to Microsoft and OpenAI. They've become complacent and messed up their opportunity. This is the opposite of what I expected, I thought Microsoft is the more complacent one.\u003c/p\u003e\n\u003cp\u003eWhy am I angry about Google not doing it though? I believe that more the power is distributed, the better it will be for normal humans. If we have just ONE of these AGI companies, it will surely lead to the destruction of everyone else. If we have TWO of these, it could lead to a war, but at least there would be hope of survival in a stable equilibrium like we had with nuclear weapons and MAD. But if we had many of these systems, say each one being cheap enough that an individual could own it, that could be a good thing for humanity.\u003c/p\u003e\n\u003cp\u003eDemocracy works because power gets distributed among people and benefits everyone. The biggest threat with these AGI companies is that this technology favours a natural monopoly. The ones with the biggest data centers could create the biggest models. It creates a threat for democracy itself. How would we solve this?\u003c/p\u003e\n\u003cp\u003eAnd even if everyone gets a super intelligent AGI of their own, I still don't see how it could possibly benefit humanity. What work would humans do?\u003c/p\u003e\n\u003ch3\u003eBargaining\u003c/h3\u003e\n\u003ch4\u003eIn the Long term\u003c/h4\u003e\n\u003cp\u003eWill humanity survive? I believe long term answers must come from physics.\u003c/p\u003e\n\u003cp\u003eThere a few possible endings that I see here:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eSilicon-based life is more efficient in all (or most) things as compared to Carbon-based life. Carbon-based life gets eliminated totally, or a small percent remains, such that it is not of any use to eliminate it, as compared to the effort it will require to do so (path dependence and activation energy). [Very much like monkeys. I certainly would not like to live in this reality.]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSilicon-based life is more efficent at certain things than Carbon-based life. A new hybrid life form evolves where work is delegated to whatever part can do it most efficiently. Muscles? Probably carbon? Head? Probably silicon? Communication? Wifi? Legs? Tires? A combination of centralised and distributed life? Humans going hunting for resources on other plants controlled by a central silicon-based super intelligence on earth (and to supply the resources back to the central super-intelligent being)? [Probably this would be okayish as long as humans keep their freedom. But even that doesn't seem likely.]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSilicon-based life is not more efficient in general than humans. Only certain AI models are used as tools where they are efficient enough. [Seems to be unlikely. GPT-4 has already shown us that it can be more efficient than humans at certain tasks]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4\u003eIn the short term\u003c/h4\u003e\n\u003cp\u003eWill there be a way with which most of humanity would save themselves from the economic disaster that lies ahead? Is it possible that something like UBI emerges out of this?\u003c/p\u003e\n\u003cp\u003eI thought a lot about it but it doesn't seem that there are any easy answers.\u003c/p\u003e\n\u003cp\u003eProbably the best I can do is to try and survive the economic disaster in the short term and keep thinking about how this will change things in the long term, especially as I get more data points in the near future.\u003c/p\u003e\n\u003ch3\u003eDepression and Acceptance\u003c/h3\u003e\n\u003cp\u003eThat's me right now.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt is so; it cannot be otherwise.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe rate at which ML research is progressing, I don't think I have enough time to do anything much before real AGI systems are created and the effects already start to show up. It is better then to plan for what I might do in case things start to go wrong.\u003c/p\u003e\n\u003cp\u003eIt may be possible that I am looking at it from the wrong angle - from a zero sum game perspective. There might be a way that lots of people benefit from this technology. But we need to be very careful and stay on the right path.\u003c/p\u003e\n\u003cp\u003eI will keep working hard to do whatever is in my power, to do whatever is the responsible thing to do, but I can only hope we all (humanity) make the right decisions and everything turns out to be better for all of us in the end.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eॐ सर्वे भवन्तु सुखिनः\u003cbr\u003e\nसर्वे सन्तु निरामयाः।\u003cbr\u003e\nसर्वे भद्राणि पश्यन्तु मा कश्चिद्दुःखभाग्भवेत।\u003cbr\u003e\nॐ शान्तिः शान्तिः शान्तिः॥\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eOpen Questions\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWhy isn't there any large scale discussions about this in the general public?\nI don't see other people getting terrified about it. Do they not realise the possibilities yet? Is everyone still in denial?\u003cbr\u003e\n(Update: While writing this I found out about \u003ca href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\"\u003ethis open letter\u003c/a\u003e calling for a puase on Giant AI experiments. While the game-theoretic analysis of the situation makes it clear that this letter will not stop anyone, but at least now I'm feeling a little less lonely in my fears. And at the same time the fact that other people are feeling the same fear that I am, is justifying and proving to me that the things I talked about are real possibilities and not just my imagination running wild.)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhy aren't economists discussing this more?\nI saw Economics Explained making a video with a thumbnail about the impending disaster but it was about the SVB crash instead of this! Do they not realise, how far reaching the consequences of this will be? If they don't yet, soon they will. I am eagerly looking forward to some more discussions about it. How would a post-AGI economy look like? Will most of humanity simply be slowly automated away and only a few 'owners' of the AGI remain? What would those owners even do? Why wouldn't the AGI eliminate them too, in time?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhy is the stock market not responding to it yet?\nIt probably is? Microsoft stock is doing better as compared to Google right now, but it's still low compared to its all time high! I would have expected huge sudden changes in the stock market, not slow ones like we're seeing now.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShould I start investing in some Tech/AI companies? Should I myself start studying ML and working on ML assisted dev tools (or an ML startup) to accelerate the pace of this automation (destruction?)?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhat principles are we supposed to follow now in this new world? What is the right thing to do now?\n(I had always assumed, as software engineers are always told, that we are 'creating value' when we develop new software. Now seeing this example at such a large scale, I am no longer certain if 'value' is being created with new technology and whether it really is the right thing to do.)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n"},"allNotesIndex":{"base_name":"notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes","notes_data":[{"id":"quotes","metadata":{"title":"Quotes","date":"2023-03-21"}},{"id":"thoughts_ai","metadata":{"title":"Thoughts about AI","date":"2023-03-30"}}],"directories":[{"base_name":"book notes","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/book notes","notes_data":[{"id":"dokkodo","metadata":{"title":"Dokkodo","date":"2023-03-23"}},{"id":"if","metadata":{"title":"\"If\"","date":"2023-03-27"}},{"id":"worry","metadata":{"title":"Worry","date":"2023-03-29"}}],"directories":[]},{"base_name":"economics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics","notes_data":[],"directories":[{"base_name":"microeconomics","full_path":"/home/runner/work/krkartikay.github.io/krkartikay.github.io/notes/economics/microeconomics","notes_data":[{"id":"index","metadata":{"title":"Principles of Microeconomics","date":"2023-03-23"}},{"id":"lecture_1","metadata":{"title":"Lecture 1","date":"2023-03-23"}},{"id":"lecture_2","metadata":{"title":"Lecture 2","date":"2023-03-23"}},{"id":"lecture_3","metadata":{"title":"Lecture 3","date":"2023-03-23"}},{"id":"lecture_4","metadata":{"title":"Lecture 4","date":"2023-04-01"}}],"directories":[]}]}]}},"__N_SSG":true},"page":"/notes/[note_id]","query":{"note_id":"thoughts_ai"},"buildId":"GUnq_kKRMA5sruqUN8iq5","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>